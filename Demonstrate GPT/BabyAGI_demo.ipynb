{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#a) BabyAGI with langchain"
      ],
      "metadata": {
        "id": "to23RSCxiuKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain\n",
        "!pip install --quiet openai\n",
        "!pip install --quiet google-search-results\n",
        "!pip install --quiet faiss-cpu\n",
        "!pip install --quiet tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX4FWh4llKR3",
        "outputId": "812a129f-48a2-41d8-c982-c5f3979edf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.6/727.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "from langchain import LLMChain, OpenAI, PromptTemplate\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import BaseLLM\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chains.base import Chain"
      ],
      "metadata": {
        "id": "QlqFgeJwkc4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore"
      ],
      "metadata": {
        "id": "FaEPkOJ1klW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining Chains"
      ],
      "metadata": {
        "id": "txK65bp4mKo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskCreationChain(LLMChain):\n",
        "    \"\"\"Chain to generates tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_creation_template = (\n",
        "            \"You are a task creation AI that uses the result of an execution agent\"\n",
        "            \" to create new tasks with the following objective: {objective},\"\n",
        "            \" The last completed task has the result: {result}.\"\n",
        "            \" This result was based on this task description: {task_description}.\"\n",
        "            \" These are incomplete tasks: {incomplete_tasks}.\"\n",
        "            \" Based on the result, create new tasks to be completed\"\n",
        "            \" by the AI system that do not overlap with incomplete tasks.\"\n",
        "            \" Return the tasks as an array.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_creation_template,\n",
        "            input_variables=[\n",
        "                \"result\",\n",
        "                \"task_description\",\n",
        "                \"incomplete_tasks\",\n",
        "                \"objective\",\n",
        "            ],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "FBum1U1ilpHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskPrioritizationChain(LLMChain):\n",
        "    \"\"\"Chain to prioritize tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_prioritization_template = (\n",
        "            \"You are a task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n",
        "            \" the following tasks: {task_names}.\"\n",
        "            \" Consider the ultimate objective of your team: {objective}.\"\n",
        "            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n",
        "            \" #. First task\"\n",
        "            \" #. Second task\"\n",
        "            \" Start the task list with number {next_task_id}.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_prioritization_template,\n",
        "            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "7iUJip3fmR7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExecutionChain(LLMChain):\n",
        "    \"\"\"Chain to execute tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        execution_template = (\n",
        "            \"You are an AI who performs one task based on the following objective: {objective}.\"\n",
        "            \" Take into account these previously completed tasks: {context}.\"\n",
        "            \" Your task: {task}.\"\n",
        "            \" Response:\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=execution_template,\n",
        "            input_variables=[\"objective\", \"context\", \"task\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "lqWE5Yh-mVHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BabyAGI Controller"
      ],
      "metadata": {
        "id": "_hCvsoaxmhba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_task(\n",
        "    task_creation_chain: LLMChain,\n",
        "    result: Dict,\n",
        "    task_description: str,\n",
        "    task_list: List[str],\n",
        "    objective: str,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Get the next task.\"\"\"\n",
        "    incomplete_tasks = \", \".join(task_list)\n",
        "    response = task_creation_chain.run(\n",
        "        result=result,\n",
        "        task_description=task_description,\n",
        "        incomplete_tasks=incomplete_tasks,\n",
        "        objective=objective,\n",
        "    )\n",
        "    new_tasks = response.split(\"\\n\")\n",
        "    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]"
      ],
      "metadata": {
        "id": "cJ2SIsCFmifC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prioritize_tasks(\n",
        "    task_prioritization_chain: LLMChain,\n",
        "    this_task_id: int,\n",
        "    task_list: List[Dict],\n",
        "    objective: str,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Prioritize tasks.\"\"\"\n",
        "    task_names = [t[\"task_name\"] for t in task_list]\n",
        "    next_task_id = int(this_task_id) + 1\n",
        "    response = task_prioritization_chain.run(\n",
        "        task_names=task_names, next_task_id=next_task_id, objective=objective\n",
        "    )\n",
        "    new_tasks = response.split(\"\\n\")\n",
        "    prioritized_task_list = []\n",
        "    for task_string in new_tasks:\n",
        "        if not task_string.strip():\n",
        "            continue\n",
        "        task_parts = task_string.strip().split(\".\", 1)\n",
        "        if len(task_parts) == 2:\n",
        "            task_id = task_parts[0].strip()\n",
        "            task_name = task_parts[1].strip()\n",
        "            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
        "    return prioritized_task_list"
      ],
      "metadata": {
        "id": "cFl78jVpmlD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n",
        "    \"\"\"Get the top k tasks based on the query.\"\"\"\n",
        "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "    if not results:\n",
        "        return []\n",
        "    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n",
        "    return [str(item.metadata[\"task\"]) for item in sorted_results]\n",
        "\n",
        "\n",
        "def execute_task(\n",
        "    vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5\n",
        ") -> str:\n",
        "    \"\"\"Execute a task.\"\"\"\n",
        "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n",
        "    return execution_chain.run(objective=objective, context=context, task=task)"
      ],
      "metadata": {
        "id": "mPzdaeHsmnJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BabyAGI(Chain, BaseModel):\n",
        "    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n",
        "\n",
        "    task_list: deque = Field(default_factory=deque)\n",
        "    task_creation_chain: TaskCreationChain = Field(...)\n",
        "    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n",
        "    execution_chain: ExecutionChain = Field(...)\n",
        "    task_id_counter: int = Field(1)\n",
        "    vectorstore: VectorStore = Field(init=False)\n",
        "    max_iterations: Optional[int] = None\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def add_task(self, task: Dict):\n",
        "        self.task_list.append(task)\n",
        "\n",
        "    def print_task_list(self):\n",
        "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        for t in self.task_list:\n",
        "            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n",
        "\n",
        "    def print_next_task(self, task: Dict):\n",
        "        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n",
        "\n",
        "    def print_task_result(self, result: str):\n",
        "        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(result)\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        return [\"objective\"]\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return []\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Run the agent.\"\"\"\n",
        "        objective = inputs[\"objective\"]\n",
        "        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n",
        "        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n",
        "        num_iters = 0\n",
        "        while True:\n",
        "            if self.task_list:\n",
        "                self.print_task_list()\n",
        "\n",
        "                # Step 1: Pull the first task\n",
        "                task = self.task_list.popleft()\n",
        "                self.print_next_task(task)\n",
        "\n",
        "                # Step 2: Execute the task\n",
        "                result = execute_task(\n",
        "                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n",
        "                )\n",
        "                this_task_id = int(task[\"task_id\"])\n",
        "                self.print_task_result(result)\n",
        "\n",
        "                # Step 3: Store the result in Pinecone\n",
        "                result_id = f\"result_{task['task_id']}\"\n",
        "                self.vectorstore.add_texts(\n",
        "                    texts=[result],\n",
        "                    metadatas=[{\"task\": task[\"task_name\"]}],\n",
        "                    ids=[result_id],\n",
        "                )\n",
        "\n",
        "                # Step 4: Create new tasks and reprioritize task list\n",
        "                new_tasks = get_next_task(\n",
        "                    self.task_creation_chain,\n",
        "                    result,\n",
        "                    task[\"task_name\"],\n",
        "                    [t[\"task_name\"] for t in self.task_list],\n",
        "                    objective,\n",
        "                )\n",
        "                for new_task in new_tasks:\n",
        "                    self.task_id_counter += 1\n",
        "                    new_task.update({\"task_id\": self.task_id_counter})\n",
        "                    self.add_task(new_task)\n",
        "                self.task_list = deque(\n",
        "                    prioritize_tasks(\n",
        "                        self.task_prioritization_chain,\n",
        "                        this_task_id,\n",
        "                        list(self.task_list),\n",
        "                        objective,\n",
        "                    )\n",
        "                )\n",
        "            num_iters += 1\n",
        "            if self.max_iterations is not None and num_iters == self.max_iterations:\n",
        "                print(\n",
        "                    \"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\"\n",
        "                )\n",
        "                break\n",
        "        return {}\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(\n",
        "        cls, llm: BaseLLM, vectorstore: VectorStore, verbose: bool = False, **kwargs\n",
        "    ) -> \"BabyAGI\":\n",
        "        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n",
        "        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)\n",
        "        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n",
        "            llm, verbose=verbose\n",
        "        )\n",
        "        execution_chain = ExecutionChain.from_llm(llm, verbose=verbose)\n",
        "        return cls(\n",
        "            task_creation_chain=task_creation_chain,\n",
        "            task_prioritization_chain=task_prioritization_chain,\n",
        "            execution_chain=execution_chain,\n",
        "            vectorstore=vectorstore,\n",
        "            **kwargs,\n",
        "        )"
      ],
      "metadata": {
        "id": "wiX6gPdKmqqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Give prompt to BabyAGI"
      ],
      "metadata": {
        "id": "EJBYKBiHmvdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#hiding api keys to preserve them\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"  \n",
        "#os.environ['SERPAPI_API_KEY'] = '4f..........' "
      ],
      "metadata": {
        "id": "9ruMcsOupim6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = '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'\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "kWDxBt8jnWkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "import faiss\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
      ],
      "metadata": {
        "id": "heKer21spYiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OBJECTIVE = \"Write an essay on best practices for prompt engineering\""
      ],
      "metadata": {
        "id": "llvvxJvVmslh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "_63-pOtwm9cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging of LLMChains\n",
        "verbose = False\n",
        "# If None, will keep on going forever\n",
        "max_iterations: Optional[int] = 3\n",
        "baby_agi = BabyAGI.from_llm(\n",
        "    llm=llm, vectorstore=vectorstore, verbose=verbose, max_iterations=max_iterations\n",
        ")"
      ],
      "metadata": {
        "id": "hYd2iOR5m_57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baby_agi({\"objective\": OBJECTIVE})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU9RveW2nCZ0",
        "outputId": "0c193685-c3eb-4fed-bb73-c21249bc72a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95m\u001b[1m\n",
            "*****TASK LIST*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\u001b[92m\u001b[1m\n",
            "*****NEXT TASK*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\u001b[93m\u001b[1m\n",
            "*****TASK RESULT*****\n",
            "\u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "1. Research best practices for prompt engineering\n",
            "2. Identify key components of prompt engineering\n",
            "3. Analyze existing prompt engineering projects\n",
            "4. Develop a plan for prompt engineering\n",
            "5. Create a timeline for prompt engineering\n",
            "6. Identify resources needed for prompt engineering\n",
            "7. Develop a budget for prompt engineering\n",
            "8. Create a checklist of tasks for prompt engineering\n",
            "9. Develop a system for tracking progress of prompt engineering\n",
            "10. Develop a system for testing prompt engineering\n",
            "11. Develop a system for troubleshooting prompt engineering\n",
            "12. Develop a system for documenting prompt engineering\n",
            "13. Develop a system for sharing prompt engineering results\n",
            "14. Develop a system for evaluating prompt engineering\n",
            "15. Develop a system for monitoring prompt engineering\n",
            "\u001b[95m\u001b[1m\n",
            "*****TASK LIST*****\n",
            "\u001b[0m\u001b[0m\n",
            "2: Identify key components of prompt engineering\n",
            "3: Analyze existing prompt engineering projects\n",
            "4: Develop a plan for prompt engineering\n",
            "5: Create a timeline for prompt engineering\n",
            "6: Identify resources needed for prompt engineering\n",
            "7: Develop a budget for prompt engineering\n",
            "8: Create a checklist of tasks for prompt engineering\n",
            "9: Develop a system for tracking progress of prompt engineering\n",
            "10: Develop a system for testing prompt engineering\n",
            "11: Develop a system for troubleshooting prompt engineering\n",
            "12: Develop a system for documenting prompt engineering\n",
            "13: Develop a system for sharing prompt engineering results\n",
            "14: Develop a system for evaluating prompt engineering\n",
            "15: Develop a system for monitoring prompt engineering\n",
            "16: Develop a system for updating prompt engineering\n",
            "17: Develop a system for archiving prompt engineering\n",
            "18: Develop a system for auditing prompt engineering\n",
            "19: Develop a system for reporting on prompt engineering\n",
            "20: Develop a system for optimizing prompt engineering\n",
            "1: Research best practices for prompt engineering\n",
            "\u001b[92m\u001b[1m\n",
            "*****NEXT TASK*****\n",
            "\u001b[0m\u001b[0m\n",
            "2: Identify key components of prompt engineering\n",
            "\u001b[93m\u001b[1m\n",
            "*****TASK RESULT*****\n",
            "\u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "Prompt engineering is the process of designing and developing user interfaces that enable users to interact with a system in an efficient and effective manner. Key components of prompt engineering include:\n",
            "\n",
            "1. User Interface Design: Designing a user interface that is intuitive and easy to use is essential for prompt engineering. This includes considering the user’s needs, the system’s capabilities, and the overall user experience.\n",
            "\n",
            "2. Usability Testing: Usability testing is an important part of prompt engineering. It involves testing the user interface with real users to ensure that it is easy to use and meets the user’s needs.\n",
            "\n",
            "3. User Feedback: Gathering user feedback is essential for prompt engineering. This feedback can be used to identify areas of improvement and make changes to the user interface.\n",
            "\n",
            "4. Documentation: Documentation is an important part of prompt engineering. It helps to ensure that the user interface is well-documented and easy to understand.\n",
            "\n",
            "5. Maintenance: Maintenance is an important part of prompt engineering. It involves regularly updating the user interface to ensure that it remains up-to-date and meets the user’s needs.\n",
            "\u001b[95m\u001b[1m\n",
            "*****TASK LIST*****\n",
            "\u001b[0m\u001b[0m\n",
            "3: Analyze existing prompt engineering projects\n",
            "4: Develop a plan for prompt engineering\n",
            "5: Create a timeline for prompt engineering\n",
            "6: Identify resources needed for prompt engineering\n",
            "7: Develop a budget for prompt engineering\n",
            "8: Create a checklist of tasks for prompt engineering\n",
            "9: Develop a system for tracking progress of prompt engineering\n",
            "10: Develop a system for testing prompt engineering\n",
            "11: Develop a system for troubleshooting prompt engineering\n",
            "12: Develop a system for documenting prompt engineering\n",
            "13: Develop a system for sharing prompt engineering results\n",
            "14: Develop a system for evaluating prompt engineering\n",
            "15: Develop a system for monitoring prompt engineering\n",
            "16: Develop a system for updating prompt engineering\n",
            "17: Develop a system for archiving prompt engineering\n",
            "18: Develop a system for auditing prompt engineering\n",
            "19: Develop a system for reporting on prompt engineering\n",
            "20: Develop a system for optimizing prompt engineering\n",
            "21: Research best practices for prompt engineering\n",
            "22: Analyze the effectiveness of existing prompt engineering projects\n",
            "23: Develop a system for measuring the success of prompt engineering projects\n",
            "24: Identify potential risks associated with prompt engineering\n",
            "25: \n",
            "\u001b[92m\u001b[1m\n",
            "*****NEXT TASK*****\n",
            "\u001b[0m\u001b[0m\n",
            "3: Analyze existing prompt engineering projects\n",
            "\u001b[93m\u001b[1m\n",
            "*****TASK RESULT*****\n",
            "\u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "Prompt engineering is a complex field that requires careful analysis and planning. In order to ensure successful projects, it is important to analyze existing prompt engineering projects to identify best practices. This analysis should include an examination of the components used, the timeline of the project, the resources allocated, and the results achieved.\n",
            "\n",
            "When analyzing existing projects, it is important to identify the key components of prompt engineering. This includes the type of prompt, the target audience, the purpose of the prompt, the design of the prompt, and the delivery method. It is also important to consider the timeline of the project, including the start and end dates, the duration of the project, and the milestones achieved. Additionally, it is important to consider the resources allocated to the project, such as the budget, personnel, and materials. Finally, it is important to consider the results achieved, such as the number of responses, the quality of the responses, and the overall success of the project.\n",
            "\n",
            "Once the analysis is complete, it is important to create a to-do list of best practices for prompt engineering. This list should include the components identified in the analysis, as well as any additional steps needed to ensure a successful project. This list should be used as a guide for future projects\n",
            "\u001b[91m\u001b[1m\n",
            "*****TASK ENDING*****\n",
            "\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'objective': 'Write an essay on best practices for prompt engineering'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}